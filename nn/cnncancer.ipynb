{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "subscription_id = 'ff71bc5a-d809-4062-bd54-01d3ea83e738'\n",
        "resource_group = '1'\n",
        "workspace_name = 'nn1'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "dataset = Dataset.get_by_name(workspace, name='data2')\n",
        "dataset.download(target_path='.', overwrite=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712739514040
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# azureml-core of version 1.0.72 or higher is required\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "subscription_id = 'ff71bc5a-d809-4062-bd54-01d3ea83e738'\n",
        "resource_group = '1'\n",
        "workspace_name = 'nn1'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "dataset = Dataset.get_by_name(workspace, name='data3')\n",
        "dataset.download(target_path='.', overwrite=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712782643178
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "local_zip = 'processed_images.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('')\n",
        "zip_ref.close()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712783393045
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.layers import Dropout, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "# Assuming the ImageDataGeneratorWithPaths class is correctly implemented above this code.\n",
        "\n",
        "def prepare_data(processed_images_dir, csv_dir='csv'):\n",
        "    # Load CSV files\n",
        "    mass_train = pd.read_csv(os.path.join(csv_dir, 'mass_case_description_train_set.csv'))\n",
        "    mass_test = pd.read_csv(os.path.join(csv_dir, 'mass_case_description_test_set.csv'))\n",
        "\n",
        "    # Combine train and test datasets\n",
        "    full_mass = pd.concat([mass_train, mass_test], axis=0)\n",
        "\n",
        "    # Sample data if needed (to reduce size for testing)\n",
        "    full_mass_sample = full_mass.sample(n=1696, random_state=42)\n",
        "\n",
        "    # Map pathology to binary labels\n",
        "    class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0}\n",
        "    full_mass_sample['labels'] = full_mass_sample['pathology'].replace(class_mapper)\n",
        "\n",
        "\n",
        "    # Correct file paths to point to the processed images\n",
        "    full_mass_sample['processed_image'] = full_mass_sample.index.to_series().apply(\n",
        "        lambda i: os.path.join(processed_images_dir, f'processed_{i}.jpg'))\n",
        "\n",
        "    return full_mass_sample\n",
        "\n",
        "def build_model():\n",
        "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)  # Binary classification with categorical crossentropy\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_model(full_mass_sample, target_size=(224, 224), batch_size=32, epochs=10):\n",
        "    num_classes = 2  # Binary classification\n",
        "    model = build_model()\n",
        "\n",
        "    # Custom learning rate scheduler\n",
        "    def lr_schedule(epoch, initial_lr=0.0001):\n",
        "        lr = initial_lr\n",
        "        if epoch > 10:\n",
        "            lr *= 0.1\n",
        "        elif epoch > 5:\n",
        "            lr *= 0.5\n",
        "        return lr\n",
        "\n",
        "    # Checkpoint callback to save model weights\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        filepath='model-best.weights.h5',\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    best_val_accuracy = -np.inf\n",
        "    best_model = None\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(full_mass_sample)):\n",
        "        print(f\"Training fold {fold + 1}/5\")\n",
        "\n",
        "        train_data = full_mass_sample.iloc[train_idx]\n",
        "        test_data = full_mass_sample.iloc[test_idx]\n",
        "\n",
        "        train_images = np.array([img_to_array(load_img(img_path, target_size=target_size)) / 255.0 for img_path in train_data['processed_image']])\n",
        "        test_images = np.array([img_to_array(load_img(img_path, target_size=target_size)) / 255.0 for img_path in test_data['processed_image']])\n",
        "        train_labels = train_data['labels']\n",
        "        test_labels = test_data['labels']\n",
        "\n",
        "        datagen.fit(train_images)\n",
        "\n",
        "        history = model.fit(\n",
        "            datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
        "            validation_data=(test_images, test_labels),\n",
        "            epochs=epochs,\n",
        "            callbacks=[LearningRateScheduler(lr_schedule), early_stopping, model_checkpoint]\n",
        "        )\n",
        "\n",
        "        val_accuracy = max(history.history['val_accuracy'])\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model = model\n",
        "\n",
        "    if best_model:\n",
        "        best_model.save('final_model.h5')\n",
        "        print(\"Best model saved to final_model.h5\")\n",
        "\n",
        "# Prepare the data\n",
        "full_mass_sample = prepare_data('processed_images/sorted_files')\n",
        "# Train the model\n",
        "train_model(full_mass_sample)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712795132152
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "ru"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}